---
title: "Mining, processing, and visualizing Twitter data using rtweet, TidyText, and tidyverse"
subtitle: ""
excerpt: "Text mining in R"
date: 2022-01-10
author: "Chinmay Deval"
draft: false
images:
series:
tags:
categories: 
layout: single
---


## Data, Data, Everywhere

We live in a day and age where data is being generated at a pace that is faster than ever! Much of this data is also unstructured, untidy, and now also dominated by text format. Lately, I have been exploring the utility of the tidytext package to process such messy natural language data and draw insights. Thanks to the open source #rstats community, processing untidy unstructured data is easier than ever!

In this short example, we will grab the Twitter data using the [rtweet](https://github.com/ropensci/rtweet) package, process it using [tidytext](https://github.com/juliasilge/tidytext) and [tidyverse](https://www.tidyverse.org/), and visualize it using ggplot and leaflet.
  

## üîç Searching for Tweets

First, let us load all the packages we will need. We will use the rtweet package to query the tweets, tidytext, and tidyverse to clean and mine the text data, and ggplot and leaflet for visualization. rtweet is an R client for accessing Twitter‚Äôs REST and stream APIs. We will use [ggthemes](https://github.com/jrnold/ggthemes) package to upgrade the theming of our plots.


```r
library(rtweet)
library(tidytext)
library(tidyverse)
library(leaflet)
library(ggthemes)
```

As an example, let‚Äôs scrape tweets that use the hashtag ‚Äò#omicron‚Äô in them. We can do this using the `search_tweet()` function from the rtweet package.


```r
tweets <- search_tweets(q = "#omicron", n = 18000,
                                      lang = "en",
                                      include_rts = FALSE)
```


## üßπ Cleaning the data
Let's peek into the raw tweet data


```r
tail(tweets$text)
```

```
## [1] "Informe Covid-19 https://t.co/pjSHHaaTXV \n#COVID19 #coronavirus #Bolivia #Vacunate #Omicron\n@eldiario_net"                                                                                                                                                                                                     
## [2] "@CNN Remember when the Biden white house tweeted this gloomy message, then this happened? \n\nIf you watch CNN you don't. Their viewers are idiots. \n\n#ListenToTheScience #TrustTheScience #Omicron #JoeBiden #Biden #WhiteHouse #CNN https://t.co/frLx5SCUg5"                                                 
## [3] "Pune: 2 persons test positive for BQ.1 sub varient of omicron\n#omicron #pune #Maharashtra #omicronvarient https://t.co/poquz55POQ"                                                                                                                                                                              
## [4] "COVID-19 is on the rise as average cases this week are more than 10 per cent higher than the previous week in countries like Japan, France, South Korea and the US. The count has dropped in China, though.\n#DIU #Covid19 #PandemicAmnesty \n#COVID19 #CovidVaccine #COVID #Omicron #Jk https://t.co/Rekkb2Weft"
## [5] "#Omicron\n#LongCovidKids\n#LongCovidKids\n#CovidIsNotOver\n#NotRecovered\n#SARSCoV2\n\nUS COVID-19 policy: ‚ÄúSocial murder‚Äù of older Americans https://t.co/DKqRgLOzYm"                                                                                                                                           
## [6] "Britons wary of discretionary Christmas grocery spend -NielsenIQ #Nielseniq #Omicron #Kantar #Tesco #Sainsbury #Asda #ChristmasGrocerySpend  https://t.co/FVhJ2fUrH7"
```

At the first glance, it is clear that there is quite a bit of data cleaning required. For example, you may want to remove URLs from scraped tweets. You may even want to get rid of emoticons, punctuation, etc. while at it. As an example, let us remove the URLs from the tweets and look at the data.



```r
tweets<- tweets %>%
  mutate(stripped_text = gsub("http.*","",text))

tail(tweets$stripped_text)
```

```
## [1] "Informe Covid-19 "                                                                                                                                                                                                                                                                        
## [2] "@CNN Remember when the Biden white house tweeted this gloomy message, then this happened? \n\nIf you watch CNN you don't. Their viewers are idiots. \n\n#ListenToTheScience #TrustTheScience #Omicron #JoeBiden #Biden #WhiteHouse #CNN "                                                 
## [3] "Pune: 2 persons test positive for BQ.1 sub varient of omicron\n#omicron #pune #Maharashtra #omicronvarient "                                                                                                                                                                              
## [4] "COVID-19 is on the rise as average cases this week are more than 10 per cent higher than the previous week in countries like Japan, France, South Korea and the US. The count has dropped in China, though.\n#DIU #Covid19 #PandemicAmnesty \n#COVID19 #CovidVaccine #COVID #Omicron #Jk "
## [5] "#Omicron\n#LongCovidKids\n#LongCovidKids\n#CovidIsNotOver\n#NotRecovered\n#SARSCoV2\n\nUS COVID-19 policy: ‚ÄúSocial murder‚Äù of older Americans "                                                                                                                                           
## [6] "Britons wary of discretionary Christmas grocery spend -NielsenIQ #Nielseniq #Omicron #Kantar #Tesco #Sainsbury #Asda #ChristmasGrocerySpend  "
```

Now that the URLs are gone, the text starts to look a bit cleaner. However, you might not want to do all the cleaning manually. Thankfully, the tidytext package comes with the `unnest_tokens()` function that works like magic and cleans up all the text. It converts the text to lowercase, gets rid of punctuation marks, and adds the unique tweet id for each occurrence of the unique word. Let us use it over our URL-stripped text data.



```r
tweets_clean <- tweets %>%
  dplyr::select(stripped_text) %>%
  unnest_tokens(word, stripped_text)
```


## üôä What is the chatter about?

Let us grab and plot the 25 most frequent words from our data.


```r
tweets_clean %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "",
      y = "Count",
      title = "Unique frequent words found in tweets")+ggthemes::theme_economist()
```

<img src="index_files/figure-html/unnamed-chunk-6-1.png" width="672" />


## üßπ More cleaning

Spot any potential problems with this plot? Looks like we need to clean up the data even more. We need to filter all the 'stop words'(e.g. in, and, a, of, etc.). Again, thanks to the inbuilt `stop_words` data included with the tidytext package, we can swiftly remove all the filler words from the text. Let's test it out with our data.

Before filtering out the stop words our tweets_clean data contains 27023 observations.


```r
tweets_clean <- tweets_clean %>%
  anti_join(stop_words, by = "word")
```

After filtering out stop words our data has 15207 observations. We can also filter out the numbers/integers and the terms related to our query (such as delta, covid19, etc.) that might mask our actual results. 


```r
tweets_clean <- tweets_clean %>%
  dplyr::mutate(word = gsub('[[:digit:]]+', '_', word)) %>% 
     dplyr::filter(!str_detect(word, "omicron|covid19|covid|delta|coronavirus|_")) 
```

Let us now recreate our previous plot with this clean data.



```r
tweets_clean %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "",
      y = "Count",
      title = "Unique frequent words found in tweets")+ggthemes::theme_economist()
```

<img src="index_files/figure-html/unnamed-chunk-9-1.png" width="672" />


## üó∫ Mapping the tweets

To find out where these tweets come from, we first need to add the latitude and longitude information for each tweet. This can be done using the `lat_lng()` function from the rtweet package.


```r
tweets_with_latlons <- rtweet::lat_lng(tweets)
```








